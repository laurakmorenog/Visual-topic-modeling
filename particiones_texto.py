# -*- coding: utf-8 -*-
"""particiones_texto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U91Kr39OIshBWJMTwmRqvLBgkC4VJTbp
"""

pip install hdbscan

!pip install bertopic

from google.colab import drive
drive.mount('/content/drive')

import nltk
nltk.download('punkt')

nltk.download('punkt', download_dir='/usr/local/nltk_data')
nltk.data.path.append('/usr/local/nltk_data')

import json
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import pandas as pd

# âœ… Configurar modelo en GPU (si estÃ¡ disponible)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# âœ… Configurar segmentador de texto en chunks con LangChain
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # TamaÃ±o mÃ¡ximo de cada fragmento en caracteres
    chunk_overlap=50  # SuperposiciÃ³n entre fragmentos para mantener coherencia
)

def process_json_and_generate_embeddings(input_json_path, output_py_path):
    """
    Procesa un archivo JSON de entrevistas, segmenta los textos en fragmentos con LangChain,
    genera embeddings con Sentence-BERT y guarda los resultados en un archivo .py.
    """

    # ðŸ”¹ Leer archivo JSON desde Google Drive
    with open(input_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    partitions_list = []
    embeddings_list = []
    original_index_list = []

    # ðŸ”¹ Procesar cada entrevista
    for idx, entry in enumerate(data):
        text = entry.get('text', '').strip()  # Extraer texto del JSON
        if not text:
            continue  # Omitir entradas vacÃ­as

        # ðŸ”¹ Segmentar el texto en fragmentos
        partitions = text_splitter.split_text(text)

        # ðŸ”¹ Generar embeddings con procesamiento en batch
        embeddings = model.encode(partitions, batch_size=8, show_progress_bar=True, convert_to_numpy=True)

        # ðŸ”¹ Guardar resultados
        partitions_list.extend(partitions)
        embeddings_list.extend(embeddings.tolist())  # Convertir a listas para JSON
        original_index_list.extend([idx] * len(partitions))

    # ðŸ”¹ Crear DataFrame con los resultados
    result_df = pd.DataFrame({
        'original_index': original_index_list,
        'partition_text': partitions_list,
        'embedding': embeddings_list
    })

    # ðŸ”¹ Guardar como archivo .py
    with open(output_py_path, 'w', encoding='utf-8') as f:
        f.write(f"data = {result_df.to_dict(orient='records')}")

    print(f"âœ… Proceso completado. Resultados guardados en {output_py_path}")

# ---- ðŸ“Œ Ejecutar el script ----
input_json_path = '/content/drive/MyDrive/Tesis/entrevistas_all_2023-03-21_14-24_05.json'  # Ruta en Google Drive
output_py_path = '/content/drive/MyDrive/Tesis/embeddings_particionados2.npy'

process_json_and_generate_embeddings(input_json_path, output_py_path)

import torch
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

# âœ… Configurar modelo en GPU (si estÃ¡ disponible)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# âœ… Configurar segmentador de texto en chunks con LangChain
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # TamaÃ±o mÃ¡ximo de cada fragmento en caracteres
    chunk_overlap=50  # SuperposiciÃ³n entre fragmentos para mantener coherencia
)

def process_npy_and_generate_embeddings(input_npy_path, output_npy_path):
    """
    Procesa un archivo .npy que contiene textos de entrevistas, los segmenta en fragmentos,
    genera embeddings con Sentence-BERT y guarda los resultados en un nuevo archivo .npy.
    """

    # ðŸ”¹ Cargar datos desde el archivo .npy
    data = np.load(input_npy_path, allow_pickle=True)

    results = []
    total_docs = len(data)

    # ðŸ”¹ Procesar cada entrevista
    for idx, entry in enumerate(data):
        text = str(entry).strip()  # Convertir a string por seguridad
        if not text:
            continue  # Omitir entradas vacÃ­as

        # ðŸ”¹ Segmentar el texto en fragmentos
        partitions = text_splitter.split_text(text)

        # ðŸ”¹ Generar embeddings en batch
        embeddings = model.encode(partitions, batch_size=8, convert_to_numpy=True)

        # ðŸ”¹ Guardar cada particiÃ³n junto con su embedding
        for part, emb in zip(partitions, embeddings):
            results.append({"original_index": idx, "partition_text": part, "embedding": emb})

        # ðŸ”¹ Mostrar progreso general
        if (idx + 1) % 100 == 0 or (idx + 1) == total_docs:  # Mostrar cada 100 documentos o al final
            print(f"ðŸ“„ Procesados {idx + 1}/{total_docs} documentos...")

    # ðŸ”¹ Guardar como archivo .npy
    np.save(output_npy_path, results, allow_pickle=True)

    print(f"âœ… Proceso completado. Resultados guardados en {output_npy_path}")

# ---- ðŸ“Œ Ejecutar el script ----
input_npy_path = '/content/drive/MyDrive/Tesis/processed_text_bigrams.npy'  # Ruta en Google Drive
output_npy_path = '/content/drive/MyDrive/Tesis/embeddings_particionados3.npy'

process_npy_and_generate_embeddings(input_npy_path, output_npy_path)

import numpy as np

# Cargar el archivo .npy
output_data = np.load('/content/drive/MyDrive/Tesis/embeddings_particionados3.npy', allow_pickle=True)