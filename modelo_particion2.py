# -*- coding: utf-8 -*-
"""modelo_particion2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1URsA6qKtZkQg5fX2m_oJ_H5n0gQ0XoG-
"""

from google.colab import drive
drive.mount('/content/drive')

"""datos completos"""

import numpy as np # Import the numpy library and alias it as np

# Cargar embeddings preexistentes
output_data = np.load('/content/drive/MyDrive/Tesis/embeddings_particionados3.npy', allow_pickle=True)
embeddings = np.array([entry['embedding'] for entry in output_data])

pip install hdbscan

!pip install bertopic

!python -m spacy download es_core_news_sm # Download the Spanish language model
import spacy
import en_core_web_sm #Import english language model
nlp = spacy.load("es_core_news_sm") # Load the downloaded model

from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech
# KeyBERT
keybert_model = KeyBERTInspired()

# Part-of-Speech
pos_model = PartOfSpeech("es_core_news_sm")

# MMR
mmr_model = MaximalMarginalRelevance(diversity=0.3)

# All representation models
representation_model = {
    "KeyBERT": keybert_model,
    # "OpenAI": openai_model,  # Uncomment if you will use OpenAI
    "MMR": mmr_model,
    "POS": pos_model
}

import pandas as pd

# Assuming output_data is a list of dictionaries with 'embedding' and 'partition_text' keys
partitions_df = pd.DataFrame(output_data)

# Extract 'partition_text' from the dictionaries and create a new column
partitions_df['partition_text'] = partitions_df[0].apply(lambda x: x['partition_text'])

# Convert the 'partition_text' column to string type before fitting the model
partitions_df['partition_text'] = partitions_df['partition_text'].astype(str)

from umap import UMAP
#from sklearn.cluster import KMeans
from hdbscan import HDBSCAN
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
#Import ClassTfidfTransformer
from bertopic.vectorizers import ClassTfidfTransformer

# Descargar stopwords en español
nltk.download('stopwords')
spanish_stopwords = stopwords.words('spanish')

# Inicializar CountVectorizer con stopwords en español
vectorizer = CountVectorizer(
    ngram_range=(1, 2),
    stop_words=spanish_stopwords,
    max_df=0.95,  # Ignora términos que aparecen en más del 95% de los documentos
    min_df=5,     # Mantiene solo términos que aparecen en al menos 5 documentos
    max_features=10_000  # Reduce el ruido manteniendo solo 10,000 términos más frecuentes
)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
umap_model = UMAP(
    n_components=10,    # Reduce la dimensionalidad a 5 para una mejor separación de temas
    random_state=42,
    metric="cosine",
    n_neighbors=15,    # Aumenta la cantidad de vecinos para hacer clusters más cohesivos
    min_dist=0.02,     # Reduce la dispersión de los puntos dentro de un cluster
    verbose=True
)


hdbscan_model = HDBSCAN(
    min_cluster_size=60,  # Reduce el tamaño mínimo de clusters para permitir más grupos
    min_samples=10,       # Permite mayor flexibilidad en la densidad del cluster
    #cluster_selection_epsilon=0.15,  # Reduce la fusión de clusters grandes
    gen_min_span_tree=True,
    prediction_data=True # Add this line
)
embeddings =  embeddings
ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)

topic_model = BERTopic(language="multilingual",
    top_n_words=30,
    n_gram_range=(1,2),
    embedding_model=embedding_model, # Uncomment this line and assign your embedding model
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer,
    ctfidf_model=ctfidf_model,
    verbose=True,
    representation_model=representation_model
).fit(partitions_df['partition_text'], embeddings=embeddings)

def topic_diversity(topic_model, top_n_words=10):
    """
    Calculates the topic diversity by measuring how unique the words are across topics.
    """
    # Initialize the list for topic words
    topic_words_list = []

    # Iterate over each topic and get its words
    for topic in topic_model.get_topic_freq().index:
        if topic != -1:  # Skip the noise topic
            topic_words = topic_model.get_topic(topic)  # Get word-score pairs for the topic
            if topic_words:  # Ensure the topic contains words
                topic_words_list.append([word for word, _ in topic_words[:top_n_words]])

    # Flatten the list of words
    all_words = [word for topic_words in topic_words_list for word in topic_words]

    # Calculate the ratio of unique words
    unique_words = set(all_words)
    diversity = len(unique_words) / len(all_words) if all_words else 0

    return diversity

# Calculate topic diversity
diversity_score = topic_diversity(topic_model)
print(f"Topic Diversity: {diversity_score}")

from sklearn.metrics import silhouette_score

# Calculate silhouette score
silhouette_avg = silhouette_score(embeddings, hdbscan_model.labels_)
print(f"Silhouette Score: {silhouette_avg}")

validity_index = hdbscan_model.relative_validity_
print(f"HDBSCAN Validity Index: {validity_index}")

topic_model.get_topic_info()

topic_model.visualize_topics()

hierarchical_topics = topic_model.hierarchical_topics(partitions_df['partition_text'])

hierarchical_topics

topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)

tree = topic_model.get_topic_tree(hierarchical_topics)

print(tree)

topic_model = topic_model.reduce_topics(
    partitions_df['partition_text'],  # Usa los textos originales
    nr_topics=44  # Número de clusters deseado
)

topic_model.get_topic_info()

# prompt: topic_model.get_topic_info() exportar

topic_info_df = topic_model.get_topic_info()
topic_info_df.to_csv('/content/drive/MyDrive/Tesis/topic_info_vf.csv', index=False)

from openai import OpenAI

client = OpenAI(api_key="sk-"put your key")
models = client.models.list()
print(models)

import pandas as pd
import openai
import os
import time  # Import the time module for waiting

# Set your OpenAI API key (replace with your actual key)
openai.api_key = "sk-"put your key"
# Load the CSV file
file_path = '/content/drive/MyDrive/Tesis/topic_info_vf.csv'  # Replace with your actual file path
df = pd.read_csv(file_path)

# Function to create a prompt and query the OpenAI API
def generate_topic_name_and_description(row):
    # Create a prompt based on the keywords in the row
    prompt = f"""
    Basado en las siguientes palabras clave:
    Representation: {row['Representation']}
    KeyBERT: {row['KeyBERT']}
    MMR: {row['MMR']}
    POS: {row['POS']}

    Por favor, proporciona un nombre adecuado para el tema y una breve descripción del mismo en español.
    """

    topic_name = ""
    description = ""
    try:
        # Call the OpenAI API with the prompt
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",  # You can change to "gpt-4" if available
            messages=[
                {"role": "system", "content": "Eres un asistente que ayuda a crear nombres y descripciones de temas en español."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200
        )

        # Extract the response text
        reply = response.choices[0].message.content

        # Post-process response to extract name and description
        lines = reply.split('\n')
        topic_name = lines[0].strip()  # Assuming the first line is the topic name
        description = ' '.join(lines[1:]).strip()  # The rest is considered the description

    except openai.RateLimitError as e:
        print(f"Rate limit exceeded. Waiting and retrying... Error: {e}")
        # Implement a simple wait. For more robust handling, use exponential backoff.
        time.sleep(60) # Wait for 60 seconds before the next attempt (you might need to adjust this)
        # Optionally, you could retry the API call here with a limited number of retries.
        # For now, we'll just return empty strings and continue with the next row.
        topic_name = f"ERROR: Rate Limit - {e}"
        description = f"ERROR: Rate Limit - {e}"
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        topic_name = f"ERROR: {e}"
        description = f"ERROR: {e}"


    return topic_name, description

# Apply the function to each row in the dataframe
df['topic_name'], df['topic_description'] = zip(*df.apply(generate_topic_name_and_description, axis=1))

# Save the results to a new Excel file
output_file = '/content/drive/MyDrive/Tesis/named_topics_bigramas_español_vf.xlsx'
df.to_excel(output_file, index=False)
print(f"Results saved to {output_file}")

pip install gensim

def topic_diversity(topic_model, top_n_words=10):
    """
    Calculates the topic diversity by measuring how unique the words are across topics.
    """
    # Initialize the list for topic words
    topic_words_list = []

    # Iterate over each topic and get its words
    for topic in topic_model.get_topic_freq().index:
        if topic != -1:  # Skip the noise topic
            topic_words = topic_model.get_topic(topic)  # Get word-score pairs for the topic
            if topic_words:  # Ensure the topic contains words
                topic_words_list.append([word for word, _ in topic_words[:top_n_words]])

    # Flatten the list of words
    all_words = [word for topic_words in topic_words_list for word in topic_words]

    # Calculate the ratio of unique words
    unique_words = set(all_words)
    diversity = len(unique_words) / len(all_words) if all_words else 0

    return diversity

# Calculate topic diversity
diversity_score = topic_diversity(topic_model)
print(f"Topic Diversity: {diversity_score}")

validity_index = hdbscan_model.relative_validity_
print(f"HDBSCAN Validity Index: {validity_index}")

!pip install --force-reinstall gensim

topic_model.get_topic_info()

# Reducir los tópicos a 36
topic_model = topic_model.reduce_topics(
    partitions_df['partition_text'],  # Usa los textos originales
    ##nr_topics=36  # Número de clusters deseado
    nr_topics=44
)
# Obtener las nuevas asignaciones de tópicos después de la reducción
document_info = topic_model.get_document_info(partitions_df['partition_text'])
topic_assignments = document_info["Topic"].values  # Obtener la columna de asignaciones

# Obtener los embeddings originales de BERTopic
# embeddings = topic_model.get_embeddings()

# Proyectar embeddings en 2D con UMAP
umap_model = umap.UMAP(n_components=2, random_state=42)
embeddings_2d = umap_model.fit_transform(embeddings)

# Filtrar solo los documentos que no sean ruido (-1)
mask = topic_assignments != -1
filtered_embeddings_2d = embeddings_2d[mask]
filtered_topic_assignments = topic_assignments[mask]

# Obtener los nuevos IDs de tópicos reducidos
new_topics = np.unique(filtered_topic_assignments)

# Obtener nombres de los nuevos tópicos reducidos
topic_labels = topic_model.topic_labels_

import numpy as np
import plotly.graph_objects as go
from scipy.stats import gaussian_kde
import umap


# Calcular la densidad de puntos con KDE
x = filtered_embeddings_2d[:, 0]
y = filtered_embeddings_2d[:, 1]
kde = gaussian_kde([x, y])
density = kde([x, y])

# Crear la figura con Plotly
fig = go.Figure()

# Agregar el gráfico de densidad con KDE
fig.add_trace(go.Histogram2dContour(
    x=x,
    y=y,
    colorscale='Portland',
    contours=dict(
        showlabels=True,
        labelfont=dict(
            family='Raleway',
            color='white'
        )
    ),
    hoverlabel=dict(
        bgcolor='white',
        bordercolor='black',
        font=dict(
            family='Raleway',
            color='black')
    ),
    line_smoothing=1.2
))

# Agregar etiquetas de los clusters en sus centroides
for topic_id in new_topics:
    topic_name = topic_labels.get(topic_id, f'Topic {topic_id}')
    centroid_x = np.mean(filtered_embeddings_2d[filtered_topic_assignments == topic_id, 0])
    centroid_y = np.mean(filtered_embeddings_2d[filtered_topic_assignments == topic_id, 1])

    fig.add_annotation(
        x=centroid_x,
        y=centroid_y,
        text=topic_name,
        showarrow=False,
        font=dict(size=12, color='black')
    )

# Configurar diseño del gráfico
fig.update_layout(
    title="Densidad de Clusters de BERTopic (36 Topics Reducidos)",
    xaxis_title="UMAP Dim 1",
    yaxis_title="UMAP Dim 2",
    width=2000,
    height=1500,
    template="plotly_white"
)

# Mostrar el gráfico
fig.show()