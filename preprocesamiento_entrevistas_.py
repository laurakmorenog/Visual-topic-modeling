# -*- coding: utf-8 -*-
"""preprocesamiento entrevistas .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-bUagR4VNf9YGdptLT_ikfoQAI4pptT
"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Tesis/entrevistas_all_2023-03-21_14-24_05.json'

import pandas as pd
import re

# Read JSON into a DataFrame
df = pd.read_json(file_path)

!python -m spacy download es_core_news_sm # Download the Spanish language model for spaCy

import re
import unicodedata
import nltk
from nltk.corpus import stopwords
import spacy

# Asegúrate de haber descargado los recursos necesarios de nltk
nltk.download('stopwords')

# Función para normalizar el texto (eliminar tildes)
def normalize_text(text):
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')

# Cargar el modelo en español de spaCy
nlp = spacy.load('es_core_news_sm')

# Obtener las stopwords del modelo de spaCy y normalizarlas
stop_words_spacy = {normalize_text(word) for word in nlp.Defaults.stop_words}

# Descargar las stopwords de NLTK y normalizarlas
stop_words_nltk = {normalize_text(word) for word in stopwords.words('spanish')}

# Combinar stopwords de NLTK y spaCy
stop_words = stop_words_spacy.union(stop_words_nltk)

# Normalizar y añadir stopwords adicionales personalizadas
additional_stopwords = {
    'test', 'ent', 'hola', 'merci', 'interrup', 'eh', 'ah', 'entrevista',
    'inad', 'aca', 'alla', 'mm', 'mmm', 'audio', 'hmm', 'interrupt',
    'doctora', 'sumerce', 'digo', 'dice', 'digamos', 'dije', 'decia',
    'decian', 'iba', 'iban', 'pa', 'cont', 'mjum', 'osea', 'eehm',
    'dud', 'eeh', 'jum', 'uhm', 'interrump', '_okey_', '_y', 'interrump',
    'eeeh', 'entonce', "'testcont", "'inf", 'ehh', 'tonces', 'gracias',
    'tra', 'etcr', 'ok', 'mjm', 'don', 'x', 'jum', 'xx', 'uste', 'xxx',
    'ta', 'eeeh', 'realmente', 'ehm', "mjm'", 'jenora', 'okey', 'ujum',
    "mjm'", 'ehhh', 'ehh', 'm', 'eme', 'tes', 'dizque', 'cuenteme', 'mjmm'
}

additional_stopwords_normalized = {normalize_text(word.lower()) for word in additional_stopwords}
stop_words.update(additional_stopwords_normalized)

# Define custom bigrams to remove (ensure lowercase)
custom_bigrams = [
    'buenas tardes', 'buenos dias', 'buenas noches', 'muchas gracias',
    'muchisimas gracias', 'ay dios', 'dios quiera', 'quiera contar',
    'quiero preguntarle', 'quiero preguntarte', 'queria preguntarle',
    'queria preguntarte', 'queria preguntar', 'queria agradecerle',
    'queria agradecerte', 'datos personales', 'datos sensibles',
    'tratamiento de datos', 'vale listo'
]

# Function to remove custom bigrams from text
def remove_bigrams(text, bigrams):
    for bigram in bigrams:
        text = text.replace(bigram, '')  # Replace each bigram with an empty string
    return text

# Improved text cleaning function without lemmatization
def clean_text(text):
    # Normalize unicode characters (remove accents)
    text = normalize_text(text)

    # Convert to lowercase
    text = text.lower()

    # Remove custom bigrams
    text = remove_bigrams(text, custom_bigrams)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Remove stopwords
    words = text.split()
    cleaned_words = [word for word in words if word not in stop_words]

    # Rejoin the words into a single string
    cleaned_text = ' '.join(cleaned_words)

    # Remove extra whitespace
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text

# Function to lemmatize the cleaned text
def lemmatize_text(text):
    # Tokenize the text using spaCy
    doc = nlp(text)

    # Apply lemmatization
    lemmatized_words = [token.lemma_ for token in doc]

    # Rejoin the words into a single string
    lemmatized_text = ' '.join(lemmatized_words)

    # Remove extra whitespace
    lemmatized_text = re.sub(r'\s+', ' ', lemmatized_text).strip()

    return lemmatized_text

# Aplicar la función de limpieza a la columna de texto
df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))

import numpy as np

# Assuming you have your DataFrame 'df' with 'id_doc' and 'cleaned_text' columns

# Convert 'cleaned_text' column to a NumPy array
cleaned_text_array = df['cleaned_text'].values

# Save the array to a .npy file
np.save('/content/drive/MyDrive/Tesis/cleaned_text2.npy', cleaned_text_array)

df

# Detect and remove duplicates based on the 'cleaned_text' column
df2 = df.drop_duplicates(subset=['cleaned_text'], keep='first')

# Print the number of rows before and after removing duplicates
print(f"Number of rows before removing duplicates: {len(df)}")
print(f"Number of rows after removing duplicates: {len(df2)}")

def lemmatize_texts(texts):
    # Disable unnecessary components for faster processing
    docs = nlp.pipe(texts, batch_size=50)
    lemmatized_texts = [' '.join([token.lemma_ for token in doc]) for doc in docs]
    return lemmatized_texts

# Apply the function to the column
df2.loc[:, 'lemmatized_text'] = lemmatize_texts(df2['cleaned_text'])

import numpy as np

# Convert the DataFrame to a NumPy array
lemmatized_array = df2.to_numpy()

# Save the NumPy array to a .npy file
np.save('/content/drive/MyDrive/Tesis/lemmatized_texts.npy', lemmatized_array)

import numpy as np

# Convert the 'lemmatized_text' column to a NumPy array
lemmatized_array = df2.to_numpy()

# Save the array as a .npy file
np.save('/content/drive/MyDrive/Tesis/lemmatized_texts.npy', lemmatized_array)

import numpy as np
import pandas as pd

# Load the .npy file into a NumPy array, enabling object loading
loaded_array = np.load('/content/drive/MyDrive/Tesis/lemmatized_texts.npy', allow_pickle=True)

# Define the columns (replace with your actual column names)
columns = ['id_doc','pages' ,'text', 'cleaned_text', 'lemmatized_text']

# Convert the NumPy array back into a DataFrame using the defined columns
df2_loaded = pd.DataFrame(loaded_array, columns=columns)

import numpy as np
import pandas as pd
import re
from collections import Counter


# Crear un DataFrame con los textos lematizados
df = df2_loaded

# Textometría: Calcular estadísticas básicas
df['word_count'] = df['lemmatized_text'].apply(lambda x: len(x.split()))
df['char_count'] = df['lemmatized_text'].apply(len)
df['sentence_count'] = df['lemmatized_text'].apply(lambda x: len(re.split(r'[.!?]', x)))
df['avg_word_length'] = df['lemmatized_text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0)

max_pages = df['pages'].max()
max_words = df['word_count'].max()

print(f"Maximum number of pages: {max_pages}")
print(f"Maximum number of words: {max_words}")

# documentos con cero numero de palabras y paginas

# Filter documents with zero pages and zero words
df_filtered = df[(df['pages'] == 1) ]

# Display the filtered DataFrame
df_filtered

# You can also save this filtered DataFrame to a new file if needed
# df_filtered.to_csv('/content/drive/MyDrive/Tesis/zero_pages_zero_words.csv', index=False)

import matplotlib.pyplot as plt

# Create the histogram
plt.figure(figsize=(10, 6))  # Adjust figure size as needed
plt.hist(df['word_count'], bins=20, edgecolor='black')  # Adjust number of bins
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.title('Distribution of Word Counts')
plt.grid(axis='y', alpha=0.75)
plt.show()

num_docs_less_than_4000 = len(df[df['word_count'] < 4000])
num_docs_less_than_8000 = len(df[df['word_count'] < 8000])
num_docs_lmore_than_16000 = len(df[df['word_count'] > 16000])
print(f"Number of documents with less than 4000 words: {num_docs_less_than_4000}")
print(f"Number of documents with less than 8000 words: {num_docs_less_than_8000}")
print(f"Number of documents with more than 160000 words: {num_docs_lmore_than_16000}")

import numpy as np
import pandas as pd
import re
from collections import Counter


# Crear un DataFrame con los textos lematizados
df = df2_loaded

# Textometría: Calcular estadísticas básicas
df['word_count'] = df['lemmatized_text'].apply(lambda x: len(x.split()))
df['char_count'] = df['lemmatized_text'].apply(len)
df['sentence_count'] = df['lemmatized_text'].apply(lambda x: len(re.split(r'[.!?]', x)))
df['avg_word_length'] = df['lemmatized_text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()) if len(x.split()) > 0 else 0)

# Función para calcular diversidad léxica (palabras únicas / total de palabras)
def lexical_diversity(text):
    words = text.split()
    return len(set(words)) / len(words) if len(words) > 0 else 0

df['lexical_diversity'] = df['lemmatized_text'].apply(lexical_diversity)

# Función para contar las palabras más comunes
def most_common_words(text, n=10):
    words = text.split()
    return Counter(words).most_common(n)

df['common_words'] = df['lemmatized_text'].apply(most_common_words)

# Estadísticas resumen para la textometría
text_stats = {
    'Total Documents': len(df),
    'Total Words': df['word_count'].sum(),
    'Average Word Count': df['word_count'].mean(),
    'Average Char Count': df['char_count'].mean(),
    'Average Sentence Count': df['sentence_count'].mean(),
    'Average Word Length': df['avg_word_length'].mean(),
    'Average Lexical Diversity': df['lexical_diversity'].mean()
}

# Imprimir estadísticas resumen
print("Textometry Summary:", text_stats)

df2

from collections import Counter
from nltk import ngrams
import nltk

# Asegúrate de haber descargado los recursos necesarios de nltk
nltk.download('punkt')

# Función para obtener un listado de todos los bigramas con frecuencia mayor a 10
def get_bigrams_with_frequency_above_threshold(cleaned_texts, threshold=10):
    """
    Genera un listado de los bigramas cuya frecuencia es mayor que un umbral dado.

    Args:
        cleaned_texts (list of str): Lista de documentos de texto ya preprocesados y limpiados.
        threshold (int): El umbral mínimo de frecuencia para incluir bigramas.

    Returns:
        dict: Un diccionario con los bigramas cuya frecuencia es mayor que el umbral.
    """
    # Generar bigramas para cada documento
    all_bigrams = []
    for text in cleaned_texts:
        # Tokenizar el texto en palabras
        tokens = text.split()
        # Generar bigramas usando nltk
        bigrams = list(ngrams(tokens, 2))
        # Filtrar bigramas que contengan la misma palabra en ambas posiciones
        bigrams = ['_'.join(bigram) for bigram in bigrams if bigram[0] != bigram[1]]
        all_bigrams.extend(bigrams)

    # Contar la frecuencia de cada bigrama
    bigram_counts = Counter(all_bigrams)

    # Filtrar bigramas con frecuencia mayor que el umbral
    bigrams_above_threshold = {bigram: count for bigram, count in bigram_counts.items() if count > threshold}

    return bigrams_above_threshold

cleaned_texts = df2_loaded['lemmatized_text']
  bigramas=get_bigrams_with_frequency_above_threshold(cleaned_texts,threshold=100)

import pandas as pd
# Convertir el diccionario de bigramas a un DataFrame
df_bigrams = pd.DataFrame(list(bigramas.items()), columns=['Bigram', 'Frequency'])

# Guardar el DataFrame en un archivo Excel
df_bigrams.to_excel('bigramas_frecuencias_lema.xlsx', index=False)

import pandas as pd

# Load the Excel file
df3 = pd.read_excel('bigramas_frecuencias_lema_filtrados.xlsx')

pip install sentence-transformers

bigramas_predefinidos = df3['Bigram'].tolist()

import re
import pandas as pd
from sentence_transformers import SentenceTransformer

# Predefined bigrams (example list)
predefined_bigrams = [
    'buenos dias', 'buenas tardes', 'muchas gracias', 'inteligencia artificial',
    'transformacion digital', 'aprendizaje automatico'
]

# Function to extract unigrams and predefined bigrams from text
def extract_unigrams_and_bigrams(text, bigrams):
    # Normalize the text
    text = text.lower()

    # Tokenize the text into words
    words = text.split()

    # List to store unigrams and found bigrams
    processed_words = []
    i = 0

    # Iterate through the words
    while i < len(words):
        # Check if the current and next word form a bigram
        if i < len(words) - 1:
            two_word_sequence = f"{words[i]} {words[i + 1]}"
            if two_word_sequence in bigrams:
                processed_words.append(two_word_sequence)  # Add the bigram
                i += 2  # Skip the next word as it's part of the bigram
                continue

        # If no bigram, add the current word (unigram)
        processed_words.append(words[i])
        i += 1

    return ' '.join(processed_words)  # Return processed text as a string

# Apply the extraction function to each text in the DataFrame
df2_loaded['processed_text'] = df2_loaded['lemmatized_text'].apply(lambda x: extract_unigrams_and_bigrams(x, bigramas_predefinidos))

# Print the processed DataFrame
#print(df2_loaded[['text', 'processed_text']])

# Save as an .npy file
processed_text_array = df2_loaded['processed_text'].to_numpy()
np.save('/content/drive/MyDrive/Tesis/processed_text_bigrams.npy', processed_text_array)

import numpy as np
import pandas as pd # Import the Pandas library
# Loading the .npy file with allow_pickle=True
loaded_array = np.load('/content/drive/MyDrive/Tesis/processed_text_bigrams.npy', allow_pickle=True)

# If you want to load the .npy file back into a DataFrame
df_loaded_2 = pd.DataFrame(loaded_array, columns=['processed_text'])

!pip install sentence-transformers  # Install the necessary library

import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

# Inicializar el modelo de Sentence-BERT utilizando GPU si está disponible
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Función para dividir el texto en chunks si excede los 500 tokens
def partition_text(text, max_tokens=500, overlap=50):
    words = text.split()  # Dividir el texto en palabras
    partitions = []

    # Si el texto tiene menos de max_tokens, no dividir
    if len(words) <= max_tokens:
        return [text]

    # Particionar en fragmentos de longitud máxima con traslape
    for i in range(0, len(words), max_tokens - overlap):
        partition = ' '.join(words[i:i + max_tokens])
        partitions.append(partition)

    return partitions

# Función para generar embeddings para los textos particionados
def generate_embeddings_for_partitions(text, max_tokens=500, overlap=50):
    partitions = partition_text(text, max_tokens=max_tokens, overlap=overlap)
    embeddings = model.encode(partitions, device=device)  # Generar embeddings para cada partición en GPU
    return partitions, embeddings

# Aplicar la función a cada documento del DataFrame y guardar particiones y embeddings
def process_dataframe(df):
    partitions_list = []
    embeddings_list = []

    for idx, text in enumerate(df['processed_text']):
        partitions, embeddings = generate_embeddings_for_partitions(text)
        partitions_list.extend(partitions)  # Guardar todas las particiones en una lista
        embeddings_list.extend(embeddings)  # Guardar todos los embeddings en una lista

    # Crear un DataFrame con todas las particiones y sus embeddings correspondientes
    partitions_df = pd.DataFrame({
        'original_index': np.repeat(df.index.values, [len(p) for p in df['processed_text'].apply(partition_text)]),
        'partition_text': partitions_list,
        'embedding': embeddings_list
    })

    return partitions_df

# Simulate calling the function (the actual dataframe 'df2' isn't available in this environment)
partitions_df = process_dataframe(df_loaded_2)

partitions_df

# Guardar las particiones y embeddings
partitions_df.to_csv("/content/drive/MyDrive/Tesis/document_partitions_2.csv", index=False)

# Guardar las particiones y embeddings
# Get embeddings from the partitions_df DataFrame
embeddings = partitions_df['embedding'].to_list()

# Guardar las particiones y embeddings
np.save("/content/drive/MyDrive/Tesis/embeddings_partitions2.npy", np.array(embeddings))


partitions_df.head()